[OSEv3:children]
masters
etcd
nodes
lb

[OSEv3:vars]

debug_level=4

ansible_user=centos
ansible_become=true
ansible_service_broker_install=false

openshift_deployment_type=origin

openshift_release="3.9"

openshift_disable_check=disk_availability,memory_availability,docker_storage,docker_image_availability,package_version

openshift_clock_enabled=true

openshift_docker_options="--selinux-enabled --insecure-registry=172.30.0.0/16 --log-opt max-size=1M --log-opt max-file=3"

openshift_enable_service_catalog=false

os_sdn_network_plugin_name='redhat/openshift-ovs-networkpolicy'
#os_sdn_network_plugin_name=redhat/openshift-ovs-subnet

#openshift_use_openshift_sdn=false
#openshift_use_flannel=true
#flannel_interface=br-ex

openshift_hosted_router_replicas=3
openshift_hosted_registry_replicas=1

openshift_node_local_quota_per_fsgroup=512Mi

openshift_public_hostname=openshift.example.com

osm_use_cockpit=true

openshift_master_api_port=8443
openshift_master_console_port=8443
openshift_master_cluster_method=native
openshift_master_cluster_hostname=openshift.example.com
openshift_master_cluster_public_hostname=openshift.example.com
openshift_master_default_subdomain=apps.example.com
openshift_master_identity_providers=[{'name': 'allow_all', 'login': 'true', 'challenge': 'true', 'kind': 'AllowAllPasswordIdentityProvider'}]

openshift_cloudprovider_kind=openstack
openshift_cloudprovider_openstack_auth_url=http://192.168.1.101:5000/v3
openshift_cloudprovider_openstack_username=admin
openshift_cloudprovider_openstack_password=openstack
#openshift_cloudprovider_openstack_region=RegionOne
openshift_cloudprovider_openstack_domain_name=Default
openshift_cloudprovider_openstack_tenant_name=lab
openshift_cloudprovider_openstack_lb_subnet_id=1106bb24-7d19-4e3b-bc14-866c82659d2a
openshift_cloudprovider_openstack_blockstorage_version=v2

#registry
openshift_hosted_registry_storage_kind=openstack
openshift_hosted_registry_storage_access_modes=['ReadWriteOnce']
openshift_hosted_registry_storage_openstack_filesystem=ext4
openshift_hosted_registry_storage_openstack_volumeID=7468e961-0c69-4fe5-b1ca-8b99aecd8119
openshift_hosted_registry_storage_volume_size=50Gi

openshift_debug_level="{{ debug_level }}"
openshift_node_debug_level="{{ node_debug_level | default(debug_level, true) }}"
openshift_master_debug_level="{{ master_debug_level | default(debug_level, true) }}"

[masters]
master[0:2].example.com

[etcd]
master[0:2].example.com

[lb]
haproxy.example.com

[nodes]
master[0:2].example.com openshift_node_labels="{'zone': 'default'}"
infra0.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
app0.example.com openshift_node_labels="{'zone': 'default'}"

systemctl.log

● origin-node.service - OpenShift Node
   Loaded: loaded (/etc/systemd/system/origin-node.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/origin-node.service.d
           └─openshift-sdn-ovs.conf
   Active: activating (start) since Tue 2018-06-05 15:34:18 -03; 1min 31s ago
     Docs: https://github.com/openshift/origin
  Process: 6960 ExecStopPost=/usr/bin/dbus-send --system --dest=uk.org.thekelleys.dnsmasq /uk/org/thekelleys/dnsmasq uk.org.thekelleys.SetDomainServers array:string: (code=exited, status=0/SUCCESS)
  Process: 6958 ExecStopPost=/usr/bin/rm /etc/dnsmasq.d/node-dnsmasq.conf (code=exited, status=0/SUCCESS)
  Process: 6964 ExecStartPre=/usr/bin/dbus-send --system --dest=uk.org.thekelleys.dnsmasq /uk/org/thekelleys/dnsmasq uk.org.thekelleys.SetDomainServers array:string:/in-addr.arpa/127.0.0.1,/cluster.local/127.0.0.1 (code=exited, status=0/SUCCESS)
  Process: 6963 ExecStartPre=/usr/bin/cp /etc/origin/node/node-dnsmasq.conf /etc/dnsmasq.d/ (code=exited, status=0/SUCCESS)
 Main PID: 6966 (openshift)
   Memory: 39.6M
   CGroup: /system.slice/origin-node.service
           └─6966 /usr/bin/openshift start node --config=/etc/origin/node/node-config.yaml --loglevel=4

Jun 05 15:35:49 infra0.example.com origin-node[6966]: I0605 15:35:49.202026    6966 openstack_instances.go:76] NodeAddresses(infra0.example.com) => [{InternalIP 10.0.0.40} {ExternalIP 192.168.1.40}]
Jun 05 15:35:49 infra0.example.com origin-node[6966]: I0605 15:35:49.203556    6966 kubelet_node_status.go:454] Recording NodeHasSufficientDisk event message for node infra0.example.com
Jun 05 15:35:49 infra0.example.com origin-node[6966]: I0605 15:35:49.203572    6966 kubelet_node_status.go:454] Recording NodeHasSufficientMemory event message for node infra0.example.com
Jun 05 15:35:49 infra0.example.com origin-node[6966]: I0605 15:35:49.203579    6966 kubelet_node_status.go:454] Recording NodeHasNoDiskPressure event message for node infra0.example.com
Jun 05 15:35:49 infra0.example.com origin-node[6966]: I0605 15:35:49.203605    6966 kubelet_node_status.go:82] Attempting to register node infra0.example.com
Jun 05 15:35:49 infra0.example.com origin-node[6966]: I0605 15:35:49.204041    6966 server.go:285] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"infra0.example.com", UID:"infra0.example.com", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeHasSufficientDisk' Node infra0.example.com status is now: NodeHasSufficientDisk
Jun 05 15:35:49 infra0.example.com origin-node[6966]: I0605 15:35:49.204054    6966 server.go:285] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"infra0.example.com", UID:"infra0.example.com", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeHasSufficientMemory' Node infra0.example.com status is now: NodeHasSufficientMemory

journalctl.log

-- Logs begin at Tue 2018-06-05 14:41:38 -03, end at Tue 2018-06-05 15:36:30 -03. --
Jun 05 15:35:23 infra0.example.com origin-node[6966]: I0605 15:35:23.754907    6966 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers.mount", but ignoring.
Jun 05 15:35:23 infra0.example.com origin-node[6966]: I0605 15:35:23.754911    6966 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers.mount"
Jun 05 15:35:23 infra0.example.com origin-node[6966]: I0605 15:35:23.754914    6966 factory.go:116] Factory "docker" was unable to handle container "/system.slice/-.mount"
Jun 05 15:35:23 infra0.example.com origin-node[6966]: I0605 15:35:23.754918    6966 factory.go:109] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
Jun 05 15:35:23 infra0.example.com origin-node[6966]: I0605 15:35:23.754922    6966 manager.go:930] ignoring container "/system.slice/-.mount"
Jun 05 15:35:23 infra0.example.com origin-node[6966]: I0605 15:35:23.754925    6966 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-user-1000.mount"
Jun 05 15:35:23 infra0.example.com origin-node[6966]: I0605 15:35:23.754929    6966 factory.go:109] Factory "systemd" can handle container "/system.slice/run-user-1000.mount", but ignoring.
Jun 05 15:35:23 infra0.example.com origin-node[6966]: I0605 15:35:23.754933    6966 manager.go:930] ignoring container "/system.slice/run-user-1000.mount"
Jun 05 15:35:23 infra0.example.com origin-node[6966]: I0605 15:35:23.754975    6966 eviction_manager.go:221] eviction manager: synchronize housekeeping
Jun 05 15:35:23 infra0.example.com origin-node[6966]: E0605 15:35:23.754987    6966 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node "infra0.example.com" not found
Jun 05 15:35:23 infra0.example.com origin-node[6966]: W0605 15:35:23.761285    6966 cni.go:171] Unable to update cni config: No networks found in /etc/cni/net.d
Jun 05 15:35:23 infra0.example.com origin-node[6966]: I0605 15:35:23.761365    6966 kubelet.go:2103] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Jun 05 15:35:23 infra0.example.com origin-node[6966]: E0605 15:35:23.761374    6966 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Jun 05 15:35:25 infra0.example.com origin-node[6966]: I0605 15:35:25.406285    6966 kubelet.go:1924] SyncLoop (housekeeping)
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.075036    6966 kubelet_node_status.go:296] Setting node annotation to enable volume controller attach/detach
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.075058    6966 openstack_instances.go:39] openstack.Instances() called
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.075066    6966 openstack_instances.go:46] Claiming to support Instances
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.207144    6966 openstack_instances.go:39] openstack.Instances() called
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.207161    6966 openstack_instances.go:46] Claiming to support Instances
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.471352    6966 kubelet_node_status.go:352] Adding node label from cloud provider: beta.kubernetes.io/instance-type=67fa6799-cc0a-4233-96eb-af2ef85e7c1d
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.471369    6966 openstack.go:555] Claiming to support Zones
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.471388    6966 openstack.go:569] Current zone is {nova }
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.471395    6966 kubelet_node_status.go:363] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/zone=nova
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.471400    6966 openstack_instances.go:39] openstack.Instances() called
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.471406    6966 openstack_instances.go:46] Claiming to support Instances
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.471410    6966 openstack_instances.go:69] NodeAddresses(infra0.example.com) called
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.610326    6966 openstack_instances.go:76] NodeAddresses(infra0.example.com) => [{InternalIP 10.0.0.40} {ExternalIP 192.168.1.40}]
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.611940    6966 kubelet_node_status.go:454] Recording NodeHasSufficientDisk event message for node infra0.example.com
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.611957    6966 kubelet_node_status.go:454] Recording NodeHasSufficientMemory event message for node infra0.example.com
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.611963    6966 kubelet_node_status.go:454] Recording NodeHasNoDiskPressure event message for node infra0.example.com
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.611974    6966 kubelet_node_status.go:82] Attempting to register node infra0.example.com
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.612369    6966 server.go:285] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"infra0.example.com", UID:"infra0.example.com", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeHasSufficientDisk' Node infra0.example.com status is now: NodeHasSufficientDisk
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.612381    6966 server.go:285] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"infra0.example.com", UID:"infra0.example.com", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeHasSufficientMemory' Node infra0.example.com status is now: NodeHasSufficientMemory
Jun 05 15:35:26 infra0.example.com origin-node[6966]: I0605 15:35:26.612387    6966 server.go:285] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"infra0.example.com", UID:"infra0.example.com", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeHasNoDiskPressure' Node infra0.example.com status is now: NodeHasNoDiskPressure
Jun 05 15:35:26 infra0.example.com origin-node[6966]: E0605 15:35:26.615719    6966 kubelet_node_status.go:106] Unable to register node "infra0.example.com" with API server: nodes "infra0.example.com" is forbidden: node "10.0.0.40" cannot modify node "infra0.example.com"
Jun 05 15:35:27 infra0.example.com origin-node[6966]: I0605 15:35:27.406255    6966 kubelet.go:1924] SyncLoop (housekeeping)
